defaults:
  - model: adalayers_tok_roberta_large
  - optimization: adam
  - tokenizer_pretrained: adalayers_roberta_large_conll
  - dataset: conll
  - hydra: common
  - _self_

optimization:
  num_workers: 4
  max_epochs: 256
  batch_size: 356
  batch_size_eval: 356
  mode: conll_ner
  early_stop_patience: 40
  lr_patience: 16
  precision: "16-mixed"
  min_delta: 0.001
  best_metric: "f1"
  optim_kwargs:
    lr: 1e-4
    weight_decay: 0.001

model:
  kwargs:
    project_dim: 128
    lambda_distribution_entropy: 0.001
    alpha_distribution: 128
    add_pos_embeddings: True
    attention_heads_num: 8
    attention_dropout_prob: 0.2
    attention_layers_num: 1
    dim_feedforward: 2048
    distribution_cutoff: 0.01

logging:
  name: ${cat:"diploma", ${dataset.name}, ${model.name}}
