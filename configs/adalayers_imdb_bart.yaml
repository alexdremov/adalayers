defaults:
  - adalayers_imdb
  - override model: adalayers_bart
  - override tokenizer_pretrained: adalayers_bart
  - _self_

optimization:
  max_epochs: 128
  batch_size: 196
  batch_size_eval: 128

model:
  kwargs:
    lambda_distribution_entropy: 0.0
    alpha_distribution: 70
    attention_dropout_prob: 0.005

wandb:
  name: ${cat:"diploma_bart", ${dataset.name}, ${model.name}}
