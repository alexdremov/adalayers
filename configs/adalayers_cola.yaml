defaults:
  - model: adalayers_roberta_large
  - optimization: adam
  - tokenizer_pretrained: adalayers_roberta_large
  - dataset: cola
  - hydra: common
  - _self_

optimization:
  max_epochs: 256
  batch_size: 600
  batch_size_eval: 600
  precision: "16-mixed"
  early_stop_patience: 30
  lr_patience: 25
  best_metric: f1

model:
  kwargs:
    project_dim: 64
    attention_heads_num: 8
    lambda_distribution_entropy: 0.0
    alpha_distribution: 64
    add_pos_embeddings: True
    distribution_cutoff: 0.01

logging:
  name: ${cat:"diploma", ${dataset.name}, ${model.name}}
