defaults:
  - adalayers_conll
  - override model: adalayers_bart
  - override tokenizer_pretrained: adalayers_bart
  - _self_

optimization:
  max_epochs: 256
  batch_size: 900
  batch_size_eval: 900
  early_stop_patience: 18
  lr_patience: 8

tokenizer_pretrained:
  add_prefix_space: True

model:
  name: adalayers_token
  kwargs:
    project_dim: 256
    lambda_distribution_entropy: 0.0
    alpha_distribution: 64
    add_pos_embeddings: True
    attention_heads_num: 16
    attention_dropout_prob: 0.2
    attention_layers_num: 2
    dim_feedforward: 1024
    classes_weights: [0.0292, 0.12183, 0.11771, 0.12196, 0.11923, 0.1215697, 0.121895, 0.123348, 0.1232325]

wandb:
  name: ${cat:"final_bart", ${dataset.name}, ${model.name}}
